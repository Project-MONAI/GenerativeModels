{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b4285e3",
   "metadata": {},
   "source": [
    "# Vector Quantized Variational Autoencoders for 3D reconstruction of images\n",
    "\n",
    "This tutorial illustrates how to use MONAI for training a Vector Quantized Variational Autoencoder (VQVAE)[1] on 3D images.\n",
    "\n",
    "Here, we will train our VQVAE model to be able to reconstruct the input images.  We will work with the Decathlon Dataset available on [MONAI](https://docs.monai.io/en/stable/apps.html#monai.apps.DecathlonDataset). In order to train faster, we will select just one of the available tasks (\"Task01_BrainTumour\").\n",
    "\n",
    "The VQVAE can also be used as a generative model if an autoregressor model (e.g., PixelCNN, Decoder Transformer) is trained on the discrete latent representations of the VQVAE bottleneck. This falls outside of the scope of this tutorial.\n",
    "\n",
    "[1] - [Oord et al. \"Neural Discrete Representation Learning\"](https://arxiv.org/abs/1711.00937)\n",
    "\n",
    "TODO: Add Open in Colab\n",
    "\n",
    "### Setup environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a4ca0e",
   "metadata": {},
   "source": [
    "### Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb14df03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from monai import transforms\n",
    "from monai.apps import DecathlonDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, Dataset\n",
    "from monai.utils import first, set_determinism\n",
    "from torch.nn import L1Loss\n",
    "from tqdm import tqdm\n",
    "\n",
    "from generative.networks.nets import VQVAE\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352bd8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility purposes set a seed\n",
    "set_determinism(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0618c17",
   "metadata": {},
   "source": [
    "### Setup a data directory\n",
    "\n",
    "Specify a `MONAI_DATA_DIRECTORY` variable, where the data will be downloaded. If not\n",
    "specified a temporary directory will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc6c2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a342ff79",
   "metadata": {},
   "source": [
    "### Setup used transforms and download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1b3bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\"]),\n",
    "        transforms.Lambdad(keys=\"image\", func=lambda x: x[:, :, :, 1]),\n",
    "        transforms.AddChanneld(keys=[\"image\"]),\n",
    "        transforms.ScaleIntensityd(keys=[\"image\"]),\n",
    "        transforms.CenterSpatialCropd(keys=[\"image\"], roi_size=[176, 224, 155]),\n",
    "        transforms.Resized(keys=[\"image\"], spatial_size=(32, 48, 32)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\"]),\n",
    "        transforms.Lambdad(keys=\"image\", func=lambda x: x[:, :, :, 1]),\n",
    "        transforms.AddChanneld(keys=[\"image\"]),\n",
    "        transforms.ScaleIntensityd(keys=[\"image\"]),\n",
    "        transforms.CenterSpatialCropd(keys=[\"image\"], roi_size=[176, 224, 155]),\n",
    "        transforms.Resized(keys=[\"image\"], spatial_size=(32, 48, 32)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399ab576",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = DecathlonDataset(\n",
    "    root_dir=root_dir,\n",
    "    task=\"Task01_BrainTumour\",\n",
    "    transform=train_transform,\n",
    "    section=\"training\",\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=8)\n",
    "\n",
    "val_ds = DecathlonDataset(\n",
    "    root_dir=root_dir,\n",
    "    task=\"Task01_BrainTumour\",\n",
    "    transform=val_transform,\n",
    "    section=\"validation\",\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c896e0a",
   "metadata": {},
   "source": [
    "### Visualize the training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a32be9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(1, 4, figsize=(10, 6))\n",
    "for i in range(4):\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    plt.imshow(train_ds[i * 20][\"image\"][0, :, :, 15].detach().cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae4f2c7",
   "metadata": {},
   "source": [
    "### Define network, optimizer and losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28d46a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")\n",
    "model = VQVAE(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    num_res_layers=2,\n",
    "    num_levels=2,\n",
    "    downsample_parameters=((2, 4, 1, 1), (2, 4, 1, 1)),\n",
    "    upsample_parameters=((2, 4, 1, 1, 0), (2, 4, 1, 1, 0)),\n",
    "    num_channels=256,\n",
    "    num_embeddings=256,\n",
    "    embedding_dim=32,\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbefd7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-4)\n",
    "l1_loss = L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe3cb3c",
   "metadata": {},
   "source": [
    "### Model training\n",
    "Here, we are training our model for 100 epochs (training time: ~60 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba11fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "val_interval = 10\n",
    "epoch_recon_loss_list = []\n",
    "epoch_quant_loss_list = []\n",
    "val_recon_epoch_loss_list = []\n",
    "intermediary_images = []\n",
    "n_example_images = 4\n",
    "\n",
    "total_start = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=110)\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    for step, batch in progress_bar:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # model outputs reconstruction and the quantization error\n",
    "        reconstruction, quantization_loss = model(images=images)\n",
    "\n",
    "        recons_loss = l1_loss(reconstruction.float(), images.float())\n",
    "\n",
    "        loss = recons_loss + quantization_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += recons_loss.item()\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            {\n",
    "                \"recons_loss\": epoch_loss / (step + 1),\n",
    "                \"quantization_loss\": quantization_loss.item() / (step + 1),\n",
    "            }\n",
    "        )\n",
    "    epoch_recon_loss_list.append(epoch_loss / (step + 1))\n",
    "    epoch_quant_loss_list.append(quantization_loss.item() / (step + 1))\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_step, batch in enumerate(val_loader, start=1):\n",
    "                images = batch[\"image\"].to(device)\n",
    "\n",
    "                reconstruction, quantization_loss = model(images=images)\n",
    "\n",
    "                # get the first sample from the first validation batch for\n",
    "                # visualizing how the training evolves\n",
    "                if val_step == 1:\n",
    "                    intermediary_images.append(reconstruction[:n_example_images, 0])\n",
    "\n",
    "                recons_loss = l1_loss(reconstruction.float(), images.float())\n",
    "\n",
    "                val_loss += recons_loss.item()\n",
    "\n",
    "        val_loss /= val_step\n",
    "        val_recon_epoch_loss_list.append(val_loss)\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"train completed, total time: {total_time}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc01098",
   "metadata": {},
   "source": [
    "### Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5f7c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.title(\"Learning Curves\", fontsize=20)\n",
    "plt.plot(np.linspace(1, n_epochs, n_epochs), epoch_recon_loss_list, color=\"C0\", linewidth=2.0, label=\"Train\")\n",
    "plt.plot(\n",
    "    np.linspace(val_interval, n_epochs, int(n_epochs / val_interval)),\n",
    "    val_recon_epoch_loss_list,\n",
    "    color=\"C1\",\n",
    "    linewidth=2.0,\n",
    "    label=\"Validation\",\n",
    ")\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.xlabel(\"Epochs\", fontsize=16)\n",
    "plt.ylabel(\"Loss\", fontsize=16)\n",
    "plt.legend(prop={\"size\": 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df78259",
   "metadata": {},
   "source": [
    "###  Plotting  evolution of reconstructed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040b52ba",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Plot every evaluation as a new line and example as columns\n",
    "val_samples = np.linspace(val_interval, n_epochs, int(n_epochs / val_interval))\n",
    "fig, ax = plt.subplots(nrows=len(val_samples), ncols=1, sharey=True)\n",
    "fig.set_size_inches(18.5, 30.5)\n",
    "for image_n in range(len(val_samples)):\n",
    "    reconstructions = intermediary_images[image_n]\n",
    "    reconstructions = np.concatenate(\n",
    "        [\n",
    "            reconstructions[0, 0, :, :, 15].cpu(),\n",
    "            np.flipud(reconstructions[0, 0, :, 24, :].cpu().T, np.flipud(reconstructions[0, 0, 15, :, :].cpu().T)),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    ax[image_n].imshow(reconstructions.cpu(), cmap=\"gray\")\n",
    "    ax[image_n].set_xticks([])\n",
    "    ax[image_n].set_yticks([])\n",
    "    ax[image_n].set_ylabel(f\"Epoch {val_samples[image_n]:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3b5cff",
   "metadata": {},
   "source": [
    "### Plotting the reconstructions from final trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709f9c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "plt.style.use(\"default\")\n",
    "plotting_image_0 = np.concatenate([images[0, 0, :, :, 15].cpu(), np.flipud(images[0, 0, :, 24, :].cpu().T)], axis=1)\n",
    "plotting_image_1 = np.concatenate([np.flipud(images[0, 0, 15, :, :].cpu().T), np.zeros((32, 32))], axis=1)\n",
    "image = np.concatenate([plotting_image_0, plotting_image_1], axis=0)\n",
    "\n",
    "ax[0].imshow(image, vmin=0, vmax=1, cmap=\"gray\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[0].title.set_text(\"Inputted Image\")\n",
    "\n",
    "plotting_image_2 = np.concatenate(\n",
    "    [reconstruction[0, 0, :, :, 15].cpu(), np.flipud(reconstruction[0, 0, :, 24, :].cpu().T)], axis=1\n",
    ")\n",
    "plotting_image_3 = np.concatenate([np.flipud(reconstruction[0, 0, 15, :, :].cpu().T), np.zeros((32, 32))], axis=1)\n",
    "reconstruction_3d = np.concatenate([plotting_image_2, plotting_image_3], axis=0)\n",
    "ax[1].imshow(reconstruction_3d, vmin=0, vmax=1, cmap=\"gray\")\n",
    "ax[1].axis(\"off\")\n",
    "ax[1].title.set_text(\"Reconstruction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ef9b14",
   "metadata": {},
   "source": [
    "### Cleanup data directory\n",
    "\n",
    "Remove directory if a temporary was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53b24f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if directory is None:\n",
    "    shutil.rmtree(root_dir)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "auto:percent,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

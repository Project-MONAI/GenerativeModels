{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc826984",
   "metadata": {},
   "source": [
    "# Vector Quantized Variational Autoencoders and Transformers with MedNIST Dataset\n",
    "\n",
    "This tutorial illustrates how to use MONAI for training a Vector Quantized Variational Autoencoder (VQVAE)[1] and a transformer model on 2D images.\n",
    "\n",
    "This is a two step process:\n",
    "- We will train our VQVAE model to be able to reconstruct the input images.\n",
    "- This will be followed by using the trained VQVAE model to encode images to feed into the transformer network to train.\n",
    "\n",
    "We will work with the MedNIST dataset available on MONAI\n",
    "(https://docs.monai.io/en/stable/apps.html#monai.apps.MedNISTDataset). In order to train faster, we will select just one of the available classes (\"HeadCT\"), resulting in a training set with 7999 2D images.\n",
    "\n",
    "[1] - [Oord et al. \"Neural Discrete Representation Learning\"](https://arxiv.org/abs/1711.00937)\n",
    "\n",
    "\n",
    "### Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2e2865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import L1Loss, CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "from monai import transforms\n",
    "from monai.apps import MedNISTDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, Dataset\n",
    "from monai.utils import first, set_determinism\n",
    "from tqdm import tqdm\n",
    "from ignite.utils import convert_tensor\n",
    "\n",
    "from generative.networks.nets import VQVAE, DecoderOnlyTransformer\n",
    "from generative.utils.ordering import Ordering\n",
    "from generative.utils.enums import OrderingType\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eecf5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility purposes set a seed\n",
    "set_determinism(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeb2157",
   "metadata": {},
   "source": [
    "### Setup a data directory and download dataset\n",
    "\n",
    "Specify a `MONAI_DATA_DIRECTORY` variable, where the data will be downloaded. If not\n",
    "specified a temporary directory will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d781fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b331a2",
   "metadata": {},
   "source": [
    "### Download training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89063f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MedNISTDataset(root_dir=root_dir, section=\"training\", download=True, seed=0)\n",
    "train_datalist = [{\"image\": item[\"image\"]} for item in train_data.data if item[\"class_name\"] == \"HeadCT\"]\n",
    "image_size = 64\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\"]),\n",
    "        transforms.EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        transforms.ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0, clip=True),\n",
    "        transforms.RandAffined(\n",
    "            keys=[\"image\"],\n",
    "            rotate_range=[(-np.pi / 36, np.pi / 36), (-np.pi / 36, np.pi / 36)],\n",
    "            translate_range=[(-1, 1), (-1, 1)],\n",
    "            scale_range=[(-0.05, 0.05), (-0.05, 0.05)],\n",
    "            spatial_size=[image_size, image_size],\n",
    "            padding_mode=\"zeros\",\n",
    "            prob=0.5,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "train_ds = Dataset(data=train_datalist, transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ce954e",
   "metadata": {},
   "source": [
    "### Visualse some examples from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510f986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3 examples from the training set\n",
    "check_data = first(train_loader)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3)\n",
    "for image_n in range(3):\n",
    "    ax[image_n].imshow(check_data[\"image\"][image_n, 0, :, :], cmap=\"gray\")\n",
    "    ax[image_n].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acda5546",
   "metadata": {},
   "source": [
    "### Download Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde9bca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = MedNISTDataset(root_dir=root_dir, section=\"validation\", download=True, seed=0)\n",
    "val_datalist = [{\"image\": item[\"image\"]} for item in train_data.data if item[\"class_name\"] == \"HeadCT\"]\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\"]),\n",
    "        transforms.EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        transforms.ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0, clip=True),\n",
    "    ]\n",
    ")\n",
    "val_ds = Dataset(data=val_datalist, transform=val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=64, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5fac10",
   "metadata": {},
   "source": [
    "## VQVAE Training\n",
    "The first step is to train a VQVAE network - once this is done we can use the trained vqvae model to encode the 2d images to generate the inputs required for the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec13fba",
   "metadata": {},
   "source": [
    "### Define network, optimizer and losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc82d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")\n",
    "vqvae_model = VQVAE(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    num_res_layers=2,\n",
    "    num_levels=2,\n",
    "    downsample_parameters=((2, 4, 1, 1), (2, 4, 1, 1)),\n",
    "    upsample_parameters=((2, 4, 1, 1, 0), (2, 4, 1, 1, 0)),\n",
    "    num_channels=(256,256),\n",
    "    num_res_channels=(256,256),\n",
    "    num_embeddings=256,\n",
    "    embedding_dim=32,\n",
    ")\n",
    "vqvae_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675d2618",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=vqvae_model.parameters(), lr=1e-4)\n",
    "l1_loss = L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ad3fd0",
   "metadata": {},
   "source": [
    "### VQVAE Model training\n",
    "We will run our model for 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a56f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "val_interval = 10\n",
    "epoch_recon_loss_list = []\n",
    "epoch_quant_loss_list = []\n",
    "val_recon_epoch_loss_list = []\n",
    "intermediary_images = []\n",
    "n_example_images = 4\n",
    "\n",
    "total_start = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    vqvae_model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=110)\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    for step, batch in progress_bar:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # model outputs reconstruction and the quantization error\n",
    "        reconstruction, quantization_loss = vqvae_model(images=images)\n",
    "\n",
    "        recons_loss = l1_loss(reconstruction.float(), images.float())\n",
    "\n",
    "        loss = recons_loss + quantization_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += recons_loss.item()\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            {\n",
    "                \"recons_loss\": epoch_loss / (step + 1),\n",
    "                \"quantization_loss\": quantization_loss.item() / (step + 1),\n",
    "            }\n",
    "        )\n",
    "    epoch_recon_loss_list.append(epoch_loss / (step + 1))\n",
    "    epoch_quant_loss_list.append(quantization_loss.item() / (step + 1))\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        vqvae_model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            k = 0\n",
    "            for val_step, batch in enumerate(val_loader, start=1):\n",
    "                k += 1\n",
    "                if k == 3:\n",
    "                    break\n",
    "                images = batch[\"image\"].to(device)\n",
    "\n",
    "                reconstruction, quantization_loss = vqvae_model(images=images)\n",
    "\n",
    "                # get the first sample from the first validation batch for\n",
    "                # visualizing how the training evolves\n",
    "                if val_step == 1:\n",
    "                    intermediary_images.append(reconstruction[:n_example_images, 0])\n",
    "\n",
    "                recons_loss = l1_loss(reconstruction.float(), images.float())\n",
    "\n",
    "                val_loss += recons_loss.item()\n",
    "\n",
    "        val_loss /= val_step\n",
    "        val_recon_epoch_loss_list.append(val_loss)\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"train completed, total time: {total_time}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d238c9",
   "metadata": {},
   "source": [
    "### VQVE Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96730fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.title(\"Learning Curves\", fontsize=20)\n",
    "plt.plot(np.linspace(1, n_epochs, n_epochs), epoch_recon_loss_list, color=\"C0\", linewidth=2.0, label=\"Train\")\n",
    "plt.plot(\n",
    "    np.linspace(val_interval, n_epochs, int(n_epochs / val_interval)),\n",
    "    val_recon_epoch_loss_list,\n",
    "    color=\"C1\",\n",
    "    linewidth=2.0,\n",
    "    label=\"Validation\",\n",
    ")\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.xlabel(\"Epochs\", fontsize=16)\n",
    "plt.ylabel(\"Loss\", fontsize=16)\n",
    "plt.legend(prop={\"size\": 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61de2f8",
   "metadata": {},
   "source": [
    "###  Plotting  evolution of reconstruction performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccef846",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Plot every evaluation as a new line and example as columns\n",
    "val_samples = np.linspace(val_interval, n_epochs, int(n_epochs / val_interval))\n",
    "fig, ax = plt.subplots(nrows=len(val_samples), ncols=1, sharey=True)\n",
    "fig.set_size_inches(18, 30)\n",
    "for image_n in range(len(val_samples)):\n",
    "    reconstructions = torch.reshape(intermediary_images[image_n], (64 * n_example_images, 64)).T\n",
    "    ax[image_n].imshow(reconstructions.cpu(), cmap=\"gray\")\n",
    "    ax[image_n].set_xticks([])\n",
    "    ax[image_n].set_yticks([])\n",
    "    ax[image_n].set_ylabel(f\"Epoch {val_samples[image_n]:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa58261",
   "metadata": {},
   "source": [
    "### Plot reconstructions of final trained vqvae model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6efa4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "ax[0].imshow(images[0, 0].detach().cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[0].title.set_text(\"Inputted Image\")\n",
    "ax[1].imshow(reconstruction[0, 0].detach().cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "ax[1].axis(\"off\")\n",
    "ax[1].title.set_text(\"Reconstruction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40490ea",
   "metadata": {},
   "source": [
    "## Transformer Training\n",
    "Now that a vqvae model has been trained, we can use this model to encode the data into its discrete latent representations. These inputs can then be flattened into a 1D sequence for the transformer to learn in an autoregressive manor.\n",
    "\n",
    "Training can be done in 2 ways:\n",
    "- Loading in the original images and then encoding these images on the fly during training using the vqvae model, the advantage of this is we can augment training data during training that is then encoded, however this will slow down training and is more memory intensive.\n",
    "- Before training the transformer we encode all the training data first and save the discrete encodings. These latent codes are then loaded and fed to the transformer for training.\n",
    "\n",
    "For this tutorial we will use the first appraoch and use the vqvae network to encode the data during the training cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca886d3e",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "We can use the same dataloader with augmentations as used for training the VQVAE model. However given the memory intensive nature of Transformer models we will need to reduce the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c888aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=8, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11cccc3",
   "metadata": {},
   "source": [
    "### Latent sequence ordering\n",
    "We need to define an ordering of which we convert our 2D latent space into a 1D sequence. For this we will use a simple raster scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95e1cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_shape = next(iter(train_loader))[\"image\"].shape[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afcb16e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Get spatial dimensions of data\n",
    "# We divide the spatial shape by 4 as the vqvae downsamples the image by a factor of 4 along each dimension\n",
    "spatial_shape = next(iter(train_loader))[\"image\"].shape[2:]\n",
    "spatial_shape = (int(spatial_shape[0]/4),int(spatial_shape[1]/4))\n",
    "\n",
    "ordering = Ordering(ordering_type=OrderingType.RASTER_SCAN.value,\n",
    "                    spatial_dims=2,\n",
    "                    dimensions=(1,) + spatial_shape)\n",
    "\n",
    "sequence_ordering = ordering.get_sequence_ordering()\n",
    "revert_sequence_ordering = ordering.get_revert_sequence_ordering()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295e1970",
   "metadata": {},
   "source": [
    "## Define Network, optimizer and losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaa850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transformer_model = DecoderOnlyTransformer(\n",
    "        num_tokens= 256,   # must be equal to num_embeddings input of VQVAE\n",
    "        max_seq_len=spatial_shape[0]*spatial_shape[1],\n",
    "        attn_layers_dim=64,\n",
    "        attn_layers_depth=12,\n",
    "        attn_layers_heads=8,\n",
    ")\n",
    "transformer_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64b1237",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=transformer_model.parameters(), lr=1e-3)\n",
    "ce_loss = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0849c3",
   "metadata": {},
   "source": [
    "First we will define a function to allow us to generate random samples from the transformer. This will allow us to keep track of training progress as well to see how samples look during the training cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedfc55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(\n",
    "    net,\n",
    "    vqvae_model,\n",
    "    starting_tokens,\n",
    "    seq_len,\n",
    "    **kwargs\n",
    "):\n",
    "    \n",
    "    progress_bar = iter(range(seq_len))\n",
    "\n",
    "    latent_seq = starting_tokens.long()\n",
    "    for _ in progress_bar:\n",
    "        # if the sequence context is growing too long we must crop it at block_size\n",
    "        if latent_seq.size(1) <= net.max_seq_len:\n",
    "            idx_cond = latent_seq\n",
    "        else:\n",
    "            idx_cond = latent_seq[:, -net.max_seq_len :]\n",
    "\n",
    "        # forward the model to get the logits for the index in the sequence\n",
    "        logits = net(x=idx_cond)\n",
    "        # pluck the logits at the final step and scale by desired temperature\n",
    "        logits = logits[:, -1, :]\n",
    "        # optionally crop the logits to only the top k options\n",
    "\n",
    "        \n",
    "        # apply softmax to convert logits to (normalized) probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # remove the chance to be sampled the BOS token\n",
    "        probs[:, vqvae_model.num_embeddings-1] = 0\n",
    "\n",
    "        # sample from the distribution\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        latent_seq = torch.cat((latent_seq, idx_next), dim=1)\n",
    "\n",
    "    latent_seq = latent_seq[:, 1:]\n",
    "            \n",
    "    return latent_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54894d1",
   "metadata": {},
   "source": [
    "### Transformer Model Training\n",
    "We will train the model for 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34364372",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "val_interval = 10\n",
    "epoch_ce_loss_list = []\n",
    "val_ce_epoch_loss_list = []\n",
    "intermediary_images = []\n",
    "vqvae_model.eval()\n",
    "\n",
    "total_start = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    transformer_model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=110)\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    for step, batch in progress_bar:\n",
    "\n",
    "        images = batch[\"image\"].to(device)\n",
    "        # Encode images using vqvae and transformer to 1D sequence\n",
    "        quantizations = vqvae_model.index_quantize(images)\n",
    "        quantizations = quantizations.reshape(quantizations.shape[0], -1)\n",
    "        quantizations = quantizations[:, sequence_ordering]\n",
    "\n",
    "        # Pad input to give start of sequence token\n",
    "        quantizations = F.pad(quantizations, (1, 0), \"constant\", 255)  # pad with 0 i.e. vocab size of vqvae\n",
    "        quantizations = quantizations.long()\n",
    "\n",
    "        quantizations_input = convert_tensor(quantizations[:, :-1], device, non_blocking=True)\n",
    "        quantizations_target = convert_tensor(quantizations[:, 1:], device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # model outputs\n",
    "        logits = transformer_model(x=quantizations_input).transpose(1, 2)\n",
    "\n",
    "        loss = ce_loss(logits, quantizations_target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            {\n",
    "                \"ce_loss\": epoch_loss / (step + 1),\n",
    "            }\n",
    "        )\n",
    "    epoch_ce_loss_list.append(epoch_loss / (step + 1))\n",
    "\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        transformer_model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_step, batch in enumerate(val_loader, start=1):\n",
    "\n",
    "                images = batch[\"image\"].to(device)\n",
    "                # Encode images using vqvae and transformer to 1D sequence\n",
    "                quantizations = vqvae_model.index_quantize(images)\n",
    "                quantizations = quantizations.reshape(quantizations.shape[0], -1)\n",
    "                quantizations = quantizations[:, sequence_ordering]\n",
    "\n",
    "                # Pad input to give start of sequence token\n",
    "                quantizations = F.pad(quantizations, (1, 0), \"constant\", 255)  # pad with 255 i.e. vocab size of vqvae\n",
    "                quantizations = quantizations.long()\n",
    "\n",
    "                quantizations_input = convert_tensor(quantizations[:, :-1], device, non_blocking=True)\n",
    "                quantizations_target = convert_tensor(quantizations[:, 1:], device, non_blocking=True)\n",
    "\n",
    "                # model outputs\n",
    "                logits = transformer_model(x=quantizations_input).transpose(1, 2)\n",
    "\n",
    "                loss = ce_loss(logits, quantizations_target)\n",
    "\n",
    "                # Generate a random sample to visualise progress\n",
    "                if val_step == 1:\n",
    "                    starting_token = 255 * torch.ones((1, 1), device=device)\n",
    "                    generated_latent = generate(transformer_model, vqvae_model, starting_token, spatial_shape[0]*spatial_shape[1])\n",
    "                    generated_latent = generated_latent[0]\n",
    "                    vqvae_latent = generated_latent[revert_sequence_ordering]\n",
    "                    vqvae_latent = vqvae_latent.reshape((1,)+spatial_shape)\n",
    "                    decoded = vqvae_model.decode_samples(vqvae_latent)\n",
    "                    intermediary_images.append(decoded[:, 0])\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= val_step\n",
    "        val_ce_epoch_loss_list.append(val_loss)\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"train completed, total time: {total_time}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1100d2c4",
   "metadata": {},
   "source": [
    "### Transformer Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd86e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.title(\"Learning Curves\", fontsize=20)\n",
    "plt.plot(np.linspace(1, n_epochs, n_epochs), epoch_ce_loss_list, color=\"C0\", linewidth=2.0, label=\"Train\")\n",
    "plt.plot(\n",
    "    np.linspace(val_interval, n_epochs, int(n_epochs / val_interval)),\n",
    "    val_ce_epoch_loss_list,\n",
    "    color=\"C1\",\n",
    "    linewidth=2.0,\n",
    "    label=\"Validation\",\n",
    ")\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.xlabel(\"Epochs\", fontsize=16)\n",
    "plt.ylabel(\"Loss\", fontsize=16)\n",
    "plt.legend(prop={\"size\": 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e4eebf",
   "metadata": {},
   "source": [
    "### Plot evoluation of Generated Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391f5417",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Plot every evaluation as a new line and example as columns\n",
    "val_samples = np.linspace(val_interval, n_epochs, int(n_epochs / val_interval))\n",
    "print(len(val_samples))\n",
    "fig, ax = plt.subplots(nrows=len(val_samples), ncols=1, sharey=True)\n",
    "fig.set_size_inches(12, 30)\n",
    "for image_n in range(len(val_samples)):\n",
    "    reconstructions = intermediary_images[image_n][0]\n",
    "    ax[image_n].imshow(reconstructions.cpu(), cmap=\"gray\")\n",
    "    ax[image_n].set_xticks([])\n",
    "    ax[image_n].set_yticks([])\n",
    "    ax[image_n].set_ylabel(f\"Epoch {val_samples[image_n]:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fffe05f",
   "metadata": {},
   "source": [
    "### Generating samples from the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdbabce",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for i in range(5):\n",
    "    starting_token = 255 * torch.ones((1, 1), device=device)\n",
    "    generated_latent = generate(transformer_model, vqvae_model, starting_token, spatial_shape[0]*spatial_shape[1])\n",
    "    generated_latent = generated_latent[0]\n",
    "    vqvae_latent = generated_latent[revert_sequence_ordering]\n",
    "    vqvae_latent = vqvae_latent.reshape((1,)+spatial_shape)\n",
    "    decoded = vqvae_model.decode_samples(vqvae_latent)\n",
    "    samples.append(decoded[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bf0adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=5)\n",
    "for i in range(5):\n",
    "    ax[i].imshow(samples[i][0].detach().cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "    ax[i].axis(\"off\")\n",
    "    ax[i].title.set_text(\"Sample \" + str(i))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82147d1f",
   "metadata": {},
   "source": [
    "### Cleanup data directory\n",
    "\n",
    "Remove directory if a temporary was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb33a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if directory is None:\n",
    "    shutil.rmtree(root_dir)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

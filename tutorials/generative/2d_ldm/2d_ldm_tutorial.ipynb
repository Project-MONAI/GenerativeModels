{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c862ce1e",
   "metadata": {},
   "source": [
    "# 2D Latent Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5501b133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add buttom with \"Open with Colab\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6801e8b",
   "metadata": {},
   "source": [
    "## Set up environment using Colab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c9c803",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[tqdm]\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3da50af",
   "metadata": {},
   "source": [
    "## Set up imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424a5eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from monai import transforms\n",
    "from monai.apps import MedNISTDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, Dataset\n",
    "from monai.utils import first, set_determinism\n",
    "from tqdm import tqdm\n",
    "\n",
    "from generative.networks.nets import AutoencoderKL, DiffusionModelUNet\n",
    "from generative.schedulers import DDPMScheduler\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a52c24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility purposes set a seed\n",
    "set_determinism(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4031e219",
   "metadata": {},
   "source": [
    "## Setup a data directory and download dataset\n",
    "Specify a MONAI_DATA_DIRECTORY variable, where the data will be downloaded. If not specified a temporary directory will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a9b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5ad19e",
   "metadata": {},
   "source": [
    "## Download the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b253ff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MedNISTDataset(root_dir=root_dir, section=\"training\", download=True, seed=0)\n",
    "train_datalist = [{\"image\": item[\"image\"]} for item in train_data.data if item[\"class_name\"] == \"Hand\"]\n",
    "image_size = 64\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\"]),\n",
    "        transforms.EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        transforms.ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0, clip=True),\n",
    "        # TODO: Change transformations\n",
    "        transforms.RandAffined(\n",
    "            keys=[\"image\"],\n",
    "            rotate_range=[(-np.pi / 36, np.pi / 36), (-np.pi / 36, np.pi / 36)],\n",
    "            translate_range=[(-1, 1), (-1, 1)],\n",
    "            scale_range=[(-0.05, 0.05), (-0.05, 0.05)],\n",
    "            spatial_size=[image_size, image_size],\n",
    "            padding_mode=\"zeros\",\n",
    "            prob=0.5,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "train_ds = Dataset(data=train_datalist, transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81f2290",
   "metadata": {},
   "source": [
    "## Visualise examples from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e725bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3 examples from the training set\n",
    "check_data = first(train_loader)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3)\n",
    "for image_n in range(3):\n",
    "    ax[image_n].imshow(check_data[\"image\"][image_n, 0, :, :], cmap=\"gray\")\n",
    "    ax[image_n].axis(\"off\")\n",
    "# TODO: remove path\n",
    "plt.savefig(\"/project/tutorials/generative/2d_ldm/hand_examples.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe1292d",
   "metadata": {},
   "source": [
    "## Download the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40efd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = MedNISTDataset(root_dir=root_dir, section=\"validation\", download=True, seed=0)\n",
    "val_datalist = [{\"image\": item[\"image\"]} for item in train_data.data if item[\"class_name\"] == \"Hand\"]\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\"]),\n",
    "        transforms.EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        transforms.ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0, clip=True),\n",
    "    ]\n",
    ")\n",
    "val_ds = Dataset(data=val_datalist, transform=val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=64, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26cfc67",
   "metadata": {},
   "source": [
    "## Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd12c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aef349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoderkl = AutoencoderKL(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    num_channels=64,\n",
    "    latent_channels=1,\n",
    "    ch_mult=(1, 2, 2),\n",
    "    num_res_blocks=1,\n",
    "    norm_num_groups=16,\n",
    "    attention_levels=(False, False, True),\n",
    ")\n",
    "\n",
    "unet = DiffusionModelUNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    num_res_blocks=1,\n",
    "    attention_resolutions=[2, 4],\n",
    "    channel_mult=[1, 2, 2],\n",
    "    model_channels=64,\n",
    "    # TODO: play with this number\n",
    "    num_heads=1,\n",
    ")\n",
    "\n",
    "scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_schedule=\"linear\",\n",
    "    beta_start=0.0015,\n",
    "    beta_end=0.0195,\n",
    ")\n",
    "\n",
    "\n",
    "# ## Train AutoencoderKL\n",
    "autoencoderkl = autoencoderkl.to(device)\n",
    "kl_weight = 1e-6\n",
    "kl_optimizer = torch.optim.Adam(autoencoderkl.parameters(), 1e-5)\n",
    "n_epochs = 50\n",
    "val_interval = 10\n",
    "kl_epoch_loss_list = []\n",
    "kl_val_epoch_loss_list = []\n",
    "intermediary_images = []\n",
    "n_example_images = 4\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    autoencoderkl.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=70)\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    for step, batch in progress_bar:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        kl_optimizer.zero_grad(set_to_none=True)\n",
    "        reconstruction, z_mu, z_sigma = autoencoderkl(images)\n",
    "\n",
    "        rec_loss = F.mse_loss(reconstruction.float(), images.float())\n",
    "\n",
    "        kl_loss = 0.5 * torch.sum(z_mu.pow(2) + z_sigma.pow(2) - torch.log(z_sigma.pow(2)) - 1, dim=[1, 2, 3])\n",
    "        kl_loss = torch.sum(kl_loss) / kl_loss.shape[0]\n",
    "\n",
    "        # TODO: Add adversarial component\n",
    "        # TODO: Add perceptual loss\n",
    "\n",
    "        loss = rec_loss + kl_weight * kl_loss\n",
    "        loss.backward()\n",
    "        kl_optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            {\n",
    "                \"loss\": epoch_loss / (step + 1),\n",
    "            }\n",
    "        )\n",
    "    kl_epoch_loss_list.append(epoch_loss / (step + 1))\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        autoencoderkl.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_step, batch in enumerate(val_loader, start=1):\n",
    "                images = batch[\"image\"].to(device)\n",
    "                kl_optimizer.zero_grad(set_to_none=True)\n",
    "                reconstruction, z_mu, z_sigma = autoencoderkl(images)\n",
    "                # TODO: Remove this\n",
    "                # Get the first sammple from the first validation batch for visualisation\n",
    "                # purposes\n",
    "                if val_step == 1:\n",
    "                    intermediary_images.append(reconstruction[:n_example_images, 0])\n",
    "\n",
    "                rec_loss = F.mse_loss(reconstruction.float(), images.float())\n",
    "\n",
    "                kl_loss = 0.5 * torch.sum(z_mu.pow(2) + z_sigma.pow(2) - torch.log(z_sigma.pow(2)) - 1, dim=[1, 2, 3])\n",
    "                kl_loss = torch.sum(kl_loss) / kl_loss.shape[0]\n",
    "\n",
    "                # TODO: Add adversarial component\n",
    "                # TODO: Add perceptual loss\n",
    "\n",
    "                loss = rec_loss + kl_weight * kl_loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= val_step\n",
    "        kl_val_epoch_loss_list.append(val_loss)\n",
    "\n",
    "        print(f\"epoch {epoch + 1} val loss: {val_loss:.4f}\")\n",
    "progress_bar.close()\n",
    "\n",
    "# ### Visualise the results from the autoencoderKL\n",
    "val_samples = np.linspace(val_interval, n_epochs, int(n_epochs / val_interval))\n",
    "fig, ax = plt.subplots(nrows=len(val_samples), ncols=1, sharey=True)\n",
    "for image_n in range(len(val_samples)):\n",
    "    reconstructions = torch.reshape(intermediary_images[image_n], (image_size * n_example_images, image_size)).T\n",
    "    ax[image_n].imshow(reconstructions.cpu(), cmap=\"gray\")\n",
    "    ax[image_n].set_xticks([])\n",
    "    ax[image_n].set_yticks([])\n",
    "    ax[image_n].set_ylabel(f\"Epoch {val_samples[image_n]:.0f}\")\n",
    "plt.savefig(\"/project/tutorials/generative/2d_ldm/autoencoderkl.png\")\n",
    "\n",
    "# ## Train Diffusion Model\n",
    "optimizer = torch.optim.Adam(unet.parameters(), lr=2.5e-5)\n",
    "# TODO: Add lr_scheduler with warm-up\n",
    "# TODO: Add EMA model\n",
    "\n",
    "unet = unet.to(device)\n",
    "n_epochs = 50\n",
    "val_interval = 5\n",
    "epoch_loss_list = []\n",
    "val_epoch_loss_list = []\n",
    "for epoch in range(n_epochs):\n",
    "    unet.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=70)\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    for step, batch in progress_bar:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # TODO: check how to deal with next commands with multi-GPU and for FL\n",
    "        with torch.no_grad():\n",
    "            z_mu, z_sigma = autoencoderkl.encode(images)\n",
    "            z = autoencoderkl.sampling(z_mu, z_sigma)\n",
    "\n",
    "        timesteps = torch.randint(0, scheduler.num_train_timesteps, (z.shape[0],), device=z.device).long()\n",
    "\n",
    "        noise = torch.randn_like(z).to(device)\n",
    "        noisy_latent = scheduler.add_noise(original_samples=z, noise=noise, timesteps=timesteps)\n",
    "        noise_pred = unet(noisy_latent, timesteps)\n",
    "\n",
    "        loss = F.mse_loss(noise_pred.float(), noise.float())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            {\n",
    "                \"loss\": epoch_loss / (step + 1),\n",
    "            }\n",
    "        )\n",
    "    epoch_loss_list.append(epoch_loss / (step + 1))\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        unet.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_step, batch in enumerate(val_loader, start=1):\n",
    "                images = batch[\"image\"].to(device)\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    z_mu, z_sigma = autoencoderkl.encode(images)\n",
    "                    z = autoencoderkl.sampling(z_mu, z_sigma)\n",
    "\n",
    "                timesteps = torch.randint(0, scheduler.num_train_timesteps, (z.shape[0],), device=z.device).long()\n",
    "\n",
    "                noise = torch.randn_like(z).to(device)\n",
    "                noisy_latent = scheduler.add_noise(original_samples=z, noise=noise, timesteps=timesteps)\n",
    "                noise_pred = unet(noisy_latent, timesteps)\n",
    "\n",
    "                loss = F.mse_loss(noise_pred.float(), noise.float())\n",
    "\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= val_step\n",
    "        val_epoch_loss_list.append(val_loss)\n",
    "        print(f\"epoch {epoch + 1} val loss: {val_loss:.4f}\")\n",
    "progress_bar.close()\n",
    "\n",
    "\n",
    "# ## Plotting sampling example\n",
    "unet.eval()\n",
    "image = torch.randn(\n",
    "    (1, 1, 64, 64),\n",
    ")\n",
    "image = image.to(device)\n",
    "scheduler.set_timesteps(num_inference_steps=1000)\n",
    "\n",
    "intermediary = []\n",
    "for t in tqdm(scheduler.timesteps, ncols=70):\n",
    "    # 1. predict noise model_output\n",
    "    with torch.no_grad():\n",
    "        z_mu, z_sigma = autoencoderkl.encode(image)\n",
    "        z = autoencoderkl.sampling(z_mu, z_sigma)\n",
    "        model_output = unet(z, torch.Tensor((t,)).to(device))\n",
    "\n",
    "    # 2. compute previous image: x_t -> x_t-1\n",
    "    r_image, _ = scheduler.step(model_output, t, z)\n",
    "    if t % 100 == 0:\n",
    "        intermediary.append(r_image)\n",
    "\n",
    "# Invert the autoencoderKL model\n",
    "decoded_images = []\n",
    "for image in intermediary:\n",
    "    decoded = autoencoderkl.decode(image)\n",
    "    decoded_images.append(decoded)\n",
    "plt.figure()\n",
    "chain = torch.cat(decoded_images, dim=-1)\n",
    "plt.style.use(\"default\")\n",
    "plt.imshow(chain[0, 0].detach().cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"/project/tutorials/generative/2d_ldm/intermediary.png\")\n",
    "\n",
    "\n",
    "# ## Plot learning curves\n",
    "plt.figure()\n",
    "plt.title(\"Learning Curves\", fontsize=20)\n",
    "plt.plot(np.linspace(1, n_epochs, n_epochs), epoch_loss_list, linewidth=2.0, label=\"Train\")\n",
    "plt.plot(\n",
    "    np.linspace(val_interval, n_epochs, int(n_epochs / val_interval)),\n",
    "    val_epoch_loss_list,\n",
    "    linewidth=2.0,\n",
    "    label=\"Validation\",\n",
    ")\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.xlabel(\"Epochs\", fontsize=16)\n",
    "plt.ylabel(\"Loss\", fontsize=16)\n",
    "plt.legend(prop={\"size\": 14})\n",
    "plt.savefig(\"/project/tutorials/generative/2d_ldm/learning_curve.png\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

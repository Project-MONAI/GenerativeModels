{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6090d00",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Anomaly Detection with Transformers\n",
    "\n",
    "This tutorial illustrates how to use MONAI to perform image-wise anomaly detection with transformers based on the method proposed in [1].\n",
    "\n",
    "We will work with the MedNIST dataset available on MONAI\n",
    "(https://docs.monai.io/en/stable/apps.html#monai.apps.MedNISTDataset). Similar to \"Experiment 2 â€“ image-wise anomaly detection on 2D synthetic data\", we will train our models on HeadCT images and check the likelihood of similar images (in-distribution) and images from other classes\n",
    "\n",
    "[1] - [Pinaya et al. \"Unsupervised brain imaging 3D anomaly detection and segmentation with transformers\"](https://doi.org/10.1016/j.media.2022.102475)\n",
    "\n",
    "\n",
    "### Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b0c79f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Copyright 2020 MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import L1Loss, CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "from monai import transforms\n",
    "from monai.apps import MedNISTDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, Dataset\n",
    "from monai.utils import first, set_determinism\n",
    "from tqdm import tqdm\n",
    "from ignite.utils import convert_tensor\n",
    "\n",
    "from generative.networks.nets import VQVAE, DecoderOnlyTransformer\n",
    "from generative.utils.ordering import Ordering\n",
    "from generative.utils.enums import OrderingType\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0ed372",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for reproducibility purposes set a seed\n",
    "set_determinism(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad40db27",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Setup a data directory and download dataset\n",
    "\n",
    "Specify a `MONAI_DATA_DIRECTORY` variable, where the data will be downloaded. If not\n",
    "specified a temporary directory will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fa255d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10054720",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Download training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db7ac32",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data = MedNISTDataset(root_dir=root_dir, section=\"training\", download=True, seed=0)\n",
    "train_datalist = [{\"image\": item[\"image\"]} for item in train_data.data if item[\"class_name\"] == \"HeadCT\"]\n",
    "image_size = 64\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\"]),\n",
    "        transforms.EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        transforms.ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0, clip=True),\n",
    "        transforms.RandAffined(\n",
    "            keys=[\"image\"],\n",
    "            rotate_range=[(-np.pi / 36, np.pi / 36), (-np.pi / 36, np.pi / 36)],\n",
    "            translate_range=[(-1, 1), (-1, 1)],\n",
    "            scale_range=[(-0.05, 0.05), (-0.05, 0.05)],\n",
    "            spatial_size=[image_size, image_size],\n",
    "            padding_mode=\"zeros\",\n",
    "            prob=0.5,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "train_ds = Dataset(data=train_datalist, transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec356258",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Visualse some examples from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d7c3dc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot 3 examples from the training set\n",
    "check_data = first(train_loader)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3)\n",
    "for image_n in range(3):\n",
    "    ax[image_n].imshow(check_data[\"image\"][image_n, 0, :, :], cmap=\"gray\")\n",
    "    ax[image_n].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d860d83a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Download Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec954b77",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val_data = MedNISTDataset(root_dir=root_dir, section=\"validation\", download=True, seed=0)\n",
    "val_datalist = [{\"image\": item[\"image\"]} for item in train_data.data if item[\"class_name\"] == \"HeadCT\"]\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\"]),\n",
    "        transforms.EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        transforms.ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0, clip=True),\n",
    "    ]\n",
    ")\n",
    "val_ds = Dataset(data=val_datalist, transform=val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=64, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09da3d54",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Vector Quantized Variational Autoencoder (VQ-VAE) Training\n",
    "\n",
    "The first step is to train a VQVAE network - once this is done we can use the trained vqvae model to encode the 2d images to generate the inputs required for the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7a91c3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Define network, optimizer and losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757d00ff",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")\n",
    "vqvae_model = VQVAE(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    num_res_layers=2,\n",
    "    num_levels=2,\n",
    "    downsample_parameters=((2, 4, 1, 1), (2, 4, 1, 1)),\n",
    "    upsample_parameters=((2, 4, 1, 1, 0), (2, 4, 1, 1, 0)),\n",
    "    num_channels=(256, 256),\n",
    "    num_res_channels=(256, 256),\n",
    "    num_embeddings=256,\n",
    "    embedding_dim=32,\n",
    ")\n",
    "vqvae_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7611f596",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=vqvae_model.parameters(), lr=1e-4)\n",
    "l1_loss = L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d81a89",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### VQVAE Model training\n",
    "We will run our model for 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7459e4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "val_interval = 10\n",
    "epoch_recon_loss_list = []\n",
    "epoch_quant_loss_list = []\n",
    "val_recon_epoch_loss_list = []\n",
    "intermediary_images = []\n",
    "n_example_images = 4\n",
    "\n",
    "total_start = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    vqvae_model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=110)\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    for step, batch in progress_bar:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # model outputs reconstruction and the quantization error\n",
    "        reconstruction, quantization_loss = vqvae_model(images=images)\n",
    "\n",
    "        recons_loss = l1_loss(reconstruction.float(), images.float())\n",
    "\n",
    "        loss = recons_loss + quantization_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += recons_loss.item()\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            {\"recons_loss\": epoch_loss / (step + 1), \"quantization_loss\": quantization_loss.item() / (step + 1)}\n",
    "        )\n",
    "    epoch_recon_loss_list.append(epoch_loss / (step + 1))\n",
    "    epoch_quant_loss_list.append(quantization_loss.item() / (step + 1))\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        vqvae_model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            k = 0\n",
    "            for val_step, batch in enumerate(val_loader, start=1):\n",
    "                k += 1\n",
    "                if k == 3:\n",
    "                    break\n",
    "                images = batch[\"image\"].to(device)\n",
    "\n",
    "                reconstruction, quantization_loss = vqvae_model(images=images)\n",
    "\n",
    "                # get the first sample from the first validation batch for\n",
    "                # visualizing how the training evolves\n",
    "                if val_step == 1:\n",
    "                    intermediary_images.append(reconstruction[:n_example_images, 0])\n",
    "\n",
    "                recons_loss = l1_loss(reconstruction.float(), images.float())\n",
    "\n",
    "                val_loss += recons_loss.item()\n",
    "\n",
    "        val_loss /= val_step\n",
    "        val_recon_epoch_loss_list.append(val_loss)\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"train completed, total time: {total_time}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff4ec88",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###  Plotting  evolution of reconstruction performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54943066",
   "metadata": {
    "lines_to_next_cell": 2,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot every evaluation as a new line and example as columns\n",
    "val_samples = np.linspace(val_interval, n_epochs, int(n_epochs / val_interval))\n",
    "fig, ax = plt.subplots(nrows=len(val_samples), ncols=1, sharey=True)\n",
    "fig.set_size_inches(18, 30)\n",
    "for image_n in range(len(val_samples)):\n",
    "    reconstructions = torch.reshape(intermediary_images[image_n], (64 * n_example_images, 64)).T\n",
    "    ax[image_n].imshow(reconstructions.cpu(), cmap=\"gray\")\n",
    "    ax[image_n].set_xticks([])\n",
    "    ax[image_n].set_yticks([])\n",
    "    ax[image_n].set_ylabel(f\"Epoch {val_samples[image_n]:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfa3270",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Plot reconstructions of final trained vqvae model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0789cfcc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "ax[0].imshow(images[0, 0].detach().cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[0].title.set_text(\"Inputted Image\")\n",
    "ax[1].imshow(reconstruction[0, 0].detach().cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "ax[1].axis(\"off\")\n",
    "ax[1].title.set_text(\"Reconstruction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773f5f43",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Autoregressive Transformer Training\n",
    "\n",
    "Now that a vqvae model has been trained, we can use this model to encode the data into its discrete latent representations. These inputs can then be flattened into a 1D sequence for the transformer to learn in an autoregressive manor.\n",
    "\n",
    "For this tutorial we will use the first appraoch and use the vqvae network to encode the data during the training cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83352d19",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Datasets\n",
    "We can use the same dataloader with augmentations as used for training the VQVAE model. However given the memory intensive nature of Transformer models we will need to reduce the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3c3a82",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=8, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f5a3cd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Latent sequence ordering\n",
    "We need to define an ordering of which we convert our 2D latent space into a 1D sequence. For this we will use a simple raster scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efab0cc5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spatial_shape = next(iter(train_loader))[\"image\"].shape[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91086e3",
   "metadata": {
    "lines_to_next_cell": 2,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get spatial dimensions of data\n",
    "# We divide the spatial shape by 4 as the vqvae downsamples the image by a factor of 4 along each dimension\n",
    "spatial_shape = next(iter(train_loader))[\"image\"].shape[2:]\n",
    "spatial_shape = (int(spatial_shape[0] / 4), int(spatial_shape[1] / 4))\n",
    "\n",
    "ordering = Ordering(ordering_type=OrderingType.RASTER_SCAN.value, spatial_dims=2, dimensions=(1,) + spatial_shape)\n",
    "\n",
    "sequence_ordering = ordering.get_sequence_ordering()\n",
    "revert_sequence_ordering = ordering.get_revert_sequence_ordering()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace09890",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define Network, optimizer and losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab1891a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transformer_model = DecoderOnlyTransformer(\n",
    "    num_tokens=256,  # must be equal to num_embeddings input of VQVAE\n",
    "    max_seq_len=spatial_shape[0] * spatial_shape[1],\n",
    "    attn_layers_dim=64,\n",
    "    attn_layers_depth=12,\n",
    "    attn_layers_heads=8,\n",
    ")\n",
    "transformer_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3cd231",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=transformer_model.parameters(), lr=1e-3)\n",
    "ce_loss = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0921fcfb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Transformer Model Training\n",
    "We will train the model for 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c32f0a9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "val_interval = 10\n",
    "epoch_ce_loss_list = []\n",
    "val_ce_epoch_loss_list = []\n",
    "intermediary_images = []\n",
    "vqvae_model.eval()\n",
    "\n",
    "total_start = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    transformer_model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=110)\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    for step, batch in progress_bar:\n",
    "\n",
    "        images = batch[\"image\"].to(device)\n",
    "        # Encode images using vqvae and transformer to 1D sequence\n",
    "        quantizations = vqvae_model.index_quantize(images)\n",
    "        quantizations = quantizations.reshape(quantizations.shape[0], -1)\n",
    "        quantizations = quantizations[:, sequence_ordering]\n",
    "\n",
    "        # Pad input to give start of sequence token\n",
    "        quantizations = F.pad(quantizations, (1, 0), \"constant\", 255)  # pad with 0 i.e. vocab size of vqvae\n",
    "        quantizations = quantizations.long()\n",
    "\n",
    "        quantizations_input = convert_tensor(quantizations[:, :-1], device, non_blocking=True)\n",
    "        quantizations_target = convert_tensor(quantizations[:, 1:], device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # model outputs\n",
    "        logits = transformer_model(x=quantizations_input).transpose(1, 2)\n",
    "\n",
    "        loss = ce_loss(logits, quantizations_target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        progress_bar.set_postfix({\"ce_loss\": epoch_loss / (step + 1)})\n",
    "    epoch_ce_loss_list.append(epoch_loss / (step + 1))\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        transformer_model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_step, batch in enumerate(val_loader, start=1):\n",
    "\n",
    "                images = batch[\"image\"].to(device)\n",
    "                # Encode images using vqvae and transformer to 1D sequence\n",
    "                quantizations = vqvae_model.index_quantize(images)\n",
    "                quantizations = quantizations.reshape(quantizations.shape[0], -1)\n",
    "                quantizations = quantizations[:, sequence_ordering]\n",
    "\n",
    "                # Pad input to give start of sequence token\n",
    "                quantizations = F.pad(quantizations, (1, 0), \"constant\", 255)  # pad with 255 i.e. vocab size of vqvae\n",
    "                quantizations = quantizations.long()\n",
    "\n",
    "                quantizations_input = convert_tensor(quantizations[:, :-1], device, non_blocking=True)\n",
    "                quantizations_target = convert_tensor(quantizations[:, 1:], device, non_blocking=True)\n",
    "\n",
    "                # model outputs\n",
    "                logits = transformer_model(x=quantizations_input).transpose(1, 2)\n",
    "\n",
    "                loss = ce_loss(logits, quantizations_target)\n",
    "\n",
    "                # Generate a random sample to visualise progress\n",
    "                if val_step == 1:\n",
    "                    starting_token = 255 * torch.ones((1, 1), device=device)\n",
    "                    generated_latent = generate(\n",
    "                        transformer_model, vqvae_model, starting_token, spatial_shape[0] * spatial_shape[1]\n",
    "                    )\n",
    "                    generated_latent = generated_latent[0]\n",
    "                    vqvae_latent = generated_latent[revert_sequence_ordering]\n",
    "                    vqvae_latent = vqvae_latent.reshape((1,) + spatial_shape)\n",
    "                    decoded = vqvae_model.decode_samples(vqvae_latent)\n",
    "                    intermediary_images.append(decoded[:, 0])\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= val_step\n",
    "        val_ce_epoch_loss_list.append(val_loss)\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"train completed, total time: {total_time}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98070e8e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Plot evoluation of Generated Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e3e3e2",
   "metadata": {
    "lines_to_next_cell": 2,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot every evaluation as a new line and example as columns\n",
    "val_samples = np.linspace(val_interval, n_epochs, int(n_epochs / val_interval))\n",
    "print(len(val_samples))\n",
    "fig, ax = plt.subplots(nrows=len(val_samples), ncols=1, sharey=True)\n",
    "fig.set_size_inches(12, 30)\n",
    "for image_n in range(len(val_samples)):\n",
    "    reconstructions = intermediary_images[image_n][0]\n",
    "    ax[image_n].imshow(reconstructions.cpu(), cmap=\"gray\")\n",
    "    ax[image_n].set_xticks([])\n",
    "    ax[image_n].set_yticks([])\n",
    "    ax[image_n].set_ylabel(f\"Epoch {val_samples[image_n]:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a4f043",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Generating samples from the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cc187d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "samples = []\n",
    "for i in range(5):\n",
    "    starting_token = 255 * torch.ones((1, 1), device=device)\n",
    "    generated_latent = generate(transformer_model, vqvae_model, starting_token, spatial_shape[0] * spatial_shape[1])\n",
    "    generated_latent = generated_latent[0]\n",
    "    vqvae_latent = generated_latent[revert_sequence_ordering]\n",
    "    vqvae_latent = vqvae_latent.reshape((1,) + spatial_shape)\n",
    "    decoded = vqvae_model.decode_samples(vqvae_latent)\n",
    "    samples.append(decoded[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d2c316",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=5)\n",
    "for i in range(5):\n",
    "    ax[i].imshow(samples[i][0].detach().cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "    ax[i].axis(\"off\")\n",
    "    ax[i].title.set_text(\"Sample \" + str(i))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "auto:percent,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}